{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Implementation and application patterns for explaining methods]()\n",
    "-------\n",
    "#### Software chapter of the \"Interpretable AI: Interpreting, Explaining and Visualizing Deep Learning\" book \n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accompanying code for the software chapter of the book \"Interpretable AI: Interpreting, Explaining and Visualizing Deep Learning\". For a more detailed understanding please have a look into the chapter: [TODO: Add link.]().\n",
    "\n",
    "-------\n",
    "\n",
    "## Section 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.models\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications.imagenet\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "imgnetutils = imp.load_source(\"utils_imagenet\", \"../utils_imagenet.py\")\n",
    "\n",
    "# We create many graphs, let's not run out of memory.\n",
    "if keras.backend.backend() == \"tensorflow\":\n",
    "    config = keras.backend.tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    keras.backend.set_session(keras.backend.tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models, data and analyzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a set of ImageNet models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose a list of models\n",
    "netnames = [\"vgg16\",\n",
    "            \"inception_v3\",\n",
    "            \"resnet50\",\n",
    "            \"densenet121\",\n",
    "            \"nasnet_large\",\n",
    "           ]           \n",
    "n_nets = len(netnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will load a specific model, load the data in the respective format and create analyzers for this model.\n",
    "\n",
    "**For a better understanding of this part we refer to the [Comparing networks on ImagenNet](imagenet_network_comparison.ipynb) notebook, from which this code segment is adopted from.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import innvestigate.analyzer.relevance_based.relevance_rule as rrules\n",
    "epsilon = 1\n",
    "\n",
    "class CEpsilonRule(rrules.EpsilonRule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CEpsilonRule, self).__init__(*args, epsilon=epsilon, **kwargs)\n",
    "\n",
    "def prepare_model_data_and_analyzers(netname):\n",
    "    # Load the model definition.\n",
    "    tmp = getattr(innvestigate.applications.imagenet, netname)\n",
    "    net = tmp(load_weights=True, load_patterns=\"relu\")\n",
    "\n",
    "    # Build the model.\n",
    "    model = keras.models.Model(inputs=net[\"in\"], outputs=net[\"sm_out\"])\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "    # Handle input depending on model and backend.\n",
    "    channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
    "    color_conversion = \"BGRtoRGB\" if net[\"color_coding\"] == \"BGR\" else None\n",
    "\n",
    "    # Get some example test set images.\n",
    "    images, label_to_class_name = eutils.get_imagenet_data(net[\"image_shape\"][0])\n",
    "\n",
    "    if not len(images):\n",
    "        raise Exception(\"Please download the example images using: \"\n",
    "                        \"'innvestigate/examples/images/wget_imagenet_2011_samples.sh'\")\n",
    "\n",
    "    patterns = net[\"patterns\"]\n",
    "    input_range = net[\"input_range\"]\n",
    "\n",
    "    noise_scale = (input_range[1]-input_range[0]) * 0.1\n",
    "\n",
    "    # Methods we use and some properties.\n",
    "    methods = [\n",
    "        # NAME                    OPT.PARAMS                POSTPROC FXN                TITLE\n",
    "        # Show input.\n",
    "        (\"input\",                 {},                       imgnetutils.image,         \"Input\"),\n",
    "\n",
    "        (\"occlusion\",             {},                       imgnetutils.heatmap,       \"Occlusion 8x8 patches\"),\n",
    "        (\"lime\",                  {},                       imgnetutils.heatmap,       \"LIME\"),\n",
    "\n",
    "        \n",
    "        # Function\n",
    "        (\"gradient\",              {\"postprocess\": \"abs\"},   imgnetutils.heatmap,       \"Gradient\"),\n",
    "        (\"smoothgrad\",            {\"augment_by_n\": 16,\n",
    "                                   \"noise_scale\": noise_scale,\n",
    "                                   \"postprocess\": \"square\"},imgnetutils.heatmap,       \"SmoothGrad\"),\n",
    "\n",
    "        # Signal\n",
    "        (\"deconvnet\",             {},                       imgnetutils.heatmap,       \"Deconvnet\"),\n",
    "        (\"guided_backprop\",       {},                       imgnetutils.heatmap,       \"Guided Backprop\",),\n",
    "        (\"pattern.net\",           {\"patterns\": patterns},   imgnetutils.bk_proj,       \"PatternNet\"),\n",
    "\n",
    "        # Interaction\n",
    "        (\"input_t_gradient\",      {},                       imgnetutils.heatmap,       \"Input * Gradient\"),\n",
    "        (\"integrated_gradients\",  {\"reference_inputs\": input_range[0],\n",
    "                                   \"steps\": 16},            imgnetutils.heatmap,       \"Integrated Gradients\"),\n",
    "        #(\"lrp.z\",                 {},\n",
    "        #                                                    imgnetutils.heatmap,       \"LRP-Z\"),\n",
    "        (\"lrp.epsilon\",           {\"epsilon\": epsilon, },\n",
    "                                                            imgnetutils.heatmap,       \"LRP-Epsilon\"),\n",
    "        (\"lrp.sequential_preset_a_flat\",{\"epsilon\": epsilon\n",
    "                                        },                  imgnetutils.heatmap,       \"LRP-PresetA proposition\"),\n",
    "        (\"deep_taylor.bounded\",   {\"low\": input_range[0],\n",
    "                                   \"high\": input_range[1]}, imgnetutils.heatmap,       \"DeepTaylor\"),\n",
    "        (\"pattern.attribution\",   {\"patterns\": patterns},   imgnetutils.heatmap,       \"PatternAttribution\"),\n",
    "    ]\n",
    "    \n",
    "    # Create model without trailing softmax\n",
    "    model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)\n",
    "\n",
    "    # Create analyzers.\n",
    "    analyzers = []\n",
    "    for method in methods:\n",
    "        try:\n",
    "            analyzer = innvestigate.create_analyzer(method[0],        # analysis method identifier\n",
    "                                                    model_wo_softmax, # model without softmax output\n",
    "                                                    **method[1])      # optional analysis parameters\n",
    "        except Exception:\n",
    "            # Not all methods work with all models.\n",
    "            analyzer = None\n",
    "        analyzers.append(analyzer)\n",
    "        \n",
    "    return (net, color_conversion, channels_first, images, label_to_class_name,\n",
    "            methods, model, model_wo_softmax, analyzers,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert code for occlusion and LIME from implementations.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_occlusion(model, x, psize):\n",
    "    A3 = np.zeros_like(x)\n",
    "    max_neuron = model.predict(x)[0].argmax()\n",
    "\n",
    "    # Occlude patch by patch and calculate activation for each patch\n",
    "    for i in range(0, net['image_shape'][0], psize):\n",
    "        for j in range(0, net['image_shape'][0], psize):\n",
    "\n",
    "            # Create image with the patch occluded\n",
    "            occluded_x = x.copy()\n",
    "            occluded_x[:, i:i+psize, j:j+psize, :] = 0\n",
    "\n",
    "            # Store activation of occluded image\n",
    "            A3[:, i:i+psize, j:j+psize, :] = model.predict(occluded_x)[0, max_neuron]\n",
    "\n",
    "    # Normalize with initial activation value\n",
    "    A3 -= model.predict(x)[0, max_neuron]\n",
    "    return A3\n",
    "    \n",
    "def run_lime(model, x, x_pp, num_samples):\n",
    "    max_neuron = model.predict(x_pp)[0].argmax()\n",
    "\n",
    "    import lime.lime_image\n",
    "    exp = lime.lime_image.LimeImageExplainer()\n",
    "    def f(tmp):\n",
    "        tmp = imgnetutils.preprocess(tmp, net)\n",
    "        return model.predict(tmp)[:, max_neuron].reshape((-1, 1))\n",
    "\n",
    "    explanation = exp.explain_instance(x[0], f, hide_color=0, num_samples=num_samples, labels=(0,), top_labels=None)\n",
    "\n",
    "    segment_id_and_weight = explanation.local_exp[0]\n",
    "    segments = explanation.segments\n",
    "\n",
    "    tmp = np.zeros_like(x)\n",
    "    for segment_id, w in segment_id_and_weight:\n",
    "        tmp[0][segments == segment_id] = (w, w, w)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile so many networks and analyzers it can take long.\n",
    "Lets cache the results on disk in case one needs to restart the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose if the intermediate values should be cached on disk.\n",
    "# Usefule if notebooks needs to be restarted\n",
    "cache_results = True\n",
    "\n",
    "def get_cache(key):\n",
    "    if not cache_results:\n",
    "        return None\n",
    "    filename = \"compare_networks_cache__%s.npy\" % key\n",
    "    if os.path.exists(filename):\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def cache(key, x):\n",
    "    if not cache_results:\n",
    "        return\n",
    "    filename = \"compare_networks_cache__%s.npy\" % key\n",
    "    np.save(filename, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we analyze each image with the different networks and different analyzers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analyses for network vgg16.\n",
      "Creating analyses for network inception_v3.\n"
     ]
    }
   ],
   "source": [
    "analyses = {}\n",
    "texts = {}\n",
    "    \n",
    "for netname in netnames:\n",
    "    print(\"Creating analyses for network {}.\".format(netname))\n",
    "\n",
    "    tmp = prepare_model_data_and_analyzers(netname)\n",
    "    (net, color_conversion, channels_first, images, label_to_class_name,\n",
    "     methods, model, model_wo_softmax, analyzers) = tmp\n",
    "\n",
    "    analysis = np.zeros([len(images), len(analyzers)]+net[\"image_shape\"]+[3])\n",
    "    text = []\n",
    "\n",
    "    for i, (x, y) in enumerate(images):\n",
    "        # Add batch axis.\n",
    "        x = x[None, :, :, :]\n",
    "        x_pp = imgnetutils.preprocess(x, net)\n",
    "\n",
    "        # Predict final activations, probabilites, and label.\n",
    "        presm = model_wo_softmax.predict_on_batch(x_pp)[0]\n",
    "        prob = model.predict_on_batch(x_pp)[0]\n",
    "        y_hat = prob.argmax()\n",
    "\n",
    "        # Save prediction info:\n",
    "        text.append((\"%s\" % label_to_class_name[y],    # ground truth label\n",
    "                     \"%.2f\" % presm.max(),             # pre-softmax logits\n",
    "                     \"%.2f\" % prob.max(),              # probabilistic softmax output  \n",
    "                     \"%s\" % label_to_class_name[y_hat] # predicted label\n",
    "                    ))\n",
    "\n",
    "        for aidx, analyzer in enumerate(analyzers):\n",
    "            cache_key = \"%s_%s_%s\" % (netname, methods[aidx][0], i)\n",
    "            cached = get_cache(cache_key)\n",
    "            if cached is not None and not methods[aidx][0] in [\"occlusion\", \"lime\"]:\n",
    "                analysis[i, aidx] = cached\n",
    "                continue\n",
    "\n",
    "            if methods[aidx][0] == \"input\":\n",
    "                # Do not analyze, but keep not preprocessed input.\n",
    "                a = x / 255\n",
    "            elif analyzer or methods[aidx][0] in [\"occlusion\", \"lime\"]:\n",
    "                # Analyze.\n",
    "                if methods[aidx][0] in \"occlusion\":\n",
    "                    a = run_occlusion(model_wo_softmax, x_pp, 8)\n",
    "                elif methods[aidx][0] in \"lime\":\n",
    "                    a = run_lime(model_wo_softmax, x, x_pp, 1000)\n",
    "                else:\n",
    "                    a = analyzer.analyze(x_pp) \n",
    "                # Apply common postprocessing, e.g., re-ordering the channels for plotting.\n",
    "                a = imgnetutils.postprocess(a, color_conversion, channels_first)\n",
    "                # Apply analysis postprocessing, e.g., creating a heatmap.\n",
    "                a = methods[aidx][2](a)\n",
    "            else:\n",
    "                a = np.zeros_like(x)\n",
    "            # Store the analysis.\n",
    "            cache(cache_key, a[0])\n",
    "            analysis[i, aidx] = a[0]\n",
    "\n",
    "        analyses[netname] = analysis\n",
    "        texts[netname] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_images = analyses[netnames[0]].shape[0]\n",
    "\n",
    "# Prepare common labels\n",
    "col_labels = [''.join(method[3]) for method in methods]\n",
    "\n",
    "for image_index in range(n_images):\n",
    "    grid = []\n",
    "    row_labels_left = []\n",
    "    row_labels_right = []\n",
    "    \n",
    "    for netname in netnames:\n",
    "        analysis, text = analyses[netname], texts[netname]\n",
    "        # Prepare the grid as rectengular list\n",
    "        grid.append([analysis[image_index, j] for j in range(analysis.shape[1])])\n",
    "        # Prepare the labels\n",
    "        label, presm, prob, pred = zip(*text)\n",
    "        label = label[image_index]\n",
    "        row_labels_left.append(('network: {}'.format(netname),'pred: {}'.format(pred[image_index])))\n",
    "        row_labels_right.append(('logit: {}'.format(presm[image_index]),'prob: {}'.format(prob[image_index])))\n",
    "\n",
    "    # Plot the analysis.\n",
    "    print(\"Image nr. {}, true label: {}\".format(image_index, label))\n",
    "    file_name = os.environ.get(\"PLOTFILENAME\", None)\n",
    "    if file_name is not None:\n",
    "        file_name = \".\".join(file_name.split(\".\")[:-1])+(\"_%i.\" % image_index)+file_name.split(\".\")[-1]\n",
    "    eutils.plot_image_grid(grid, row_labels_left, row_labels_right, col_labels,\n",
    "                           file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This figures show the analysis regarding the *actually predicted* class as computed by the selected analyzers. Each column shows the visualized results for different analyzers and each row shows the analyses wrt to one input sample. To the left of each row, the ground truth label `label` and the predicted label `pred` are show. To the right, the model's probabilistic (softmax) output is shown as `prob` and the logit output just before the terminating softmax layer as `logit`. Note that all analyses have been performed based on the logit output (layer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
